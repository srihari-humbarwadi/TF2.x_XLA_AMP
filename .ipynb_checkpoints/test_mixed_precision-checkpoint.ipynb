{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 without XLA compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow:  2.2.0-rc3\n",
      "\n",
      "Compute dtype:  float32\n",
      "Variable dtype:  float32\n",
      "\n",
      "internal layer output dtype:  <dtype: 'float32'>\n",
      "final layer output dtype:  <dtype: 'float32'> \n",
      "\n",
      "\n",
      "Batch size:  128\n",
      "\n",
      "warmup .fit run\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4609\n",
      "\n",
      "\n",
      "\n",
      "main .fit run\n",
      "39/39 [==============================] - 16s 418ms/step - loss: 2.4208\n",
      "\n",
      "warmup custom loop run\n",
      "\n",
      "\n",
      "main custom loop run\n",
      "\n",
      "[1].[2].[3].[4].[5].[6].[7].[8].[9].[10].[11].[12].[13].[14].[15].[16].[17].[18].[19].[20].[21].[22].[23].[24].[25].[26].[27].[28].[29].[30].[31].[32].[33].[34].[35].[36].[37].[38].[39].\n",
      "\n",
      ".fit method training time: 16.78\n",
      "custom training loop training time: 16.44\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow: ', tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('float32')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "print('\\nCompute dtype: ', policy.compute_dtype)\n",
    "print('Variable dtype: ', policy.variable_dtype)\n",
    "\n",
    "def _generate_random_image(h, w):\n",
    "    image = tf.random.uniform(shape=[h, w, 3], maxval=1)\n",
    "    return image\n",
    "\n",
    "def _generate_random_label(num_classes):\n",
    "    return tf.random.uniform(shape=[], maxval=10)\n",
    "\n",
    "def _get_data(_id):\n",
    "    image = _generate_random_image(256, 256)\n",
    "    label = _generate_random_label(10)\n",
    "    return image, label\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "num_samples = 5000\n",
    "train_steps = num_samples // batch_size\n",
    "\n",
    "\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "dataset = tf.data.Dataset.range(num_samples)\n",
    "dataset = dataset.map(_get_data, num_parallel_calls=autotune) \\\n",
    "                 .batch(batch_size, drop_remainder=True) \\\n",
    "                 .repeat() \\\n",
    "                 .prefetch(autotune)\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, pooling='avg')\n",
    "x = tf.keras.layers.Dense(10)(base_model.output)\n",
    "predictions = tf.keras.layers.Activation('softmax', dtype=tf.float32)(x)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "print('\\ninternal layer output dtype: ', model.layers[21].output.dtype)\n",
    "print('final layer output dtype: ', model.output.dtype, '\\n')\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print('\\nBatch size: ', batch_size)\n",
    "\n",
    "print('\\nwarmup .fit run')\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=1)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('\\nmain .fit run')\n",
    "start_time = time()\n",
    "\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=train_steps)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "\n",
    "\n",
    "del optimizer\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "def train(num_steps):\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(image, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(image, training=True)\n",
    "            loss = tf.losses.sparse_categorical_crossentropy(labels, outputs)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        return loss\n",
    "    \n",
    "    print('\\nwarmup custom loop run\\n')\n",
    "    for image, labels in dataset.take(1):\n",
    "        _ = train_step(image, labels)\n",
    "\n",
    "    print('\\nmain custom loop run\\n')\n",
    "    start_time = time()\n",
    "    for step, (image, labels) in enumerate(dataset):\n",
    "        _ = train_step(image, labels)\n",
    "        print('[{}]'.format(step+1), end='.')\n",
    "        if step+1 == num_steps: break\n",
    "    end_time = time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "time = train(train_steps)\n",
    "\n",
    "print('\\n\\n.fit method training time: {:.2f}'.format(end_time - start_time))\n",
    "print('custom training loop training time: {:.2f}'.format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset kernel\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 with XLA compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow:  2.2.0-rc3\n",
      "\n",
      "Compute dtype:  float32\n",
      "Variable dtype:  float32\n",
      "\n",
      "internal layer output dtype:  <dtype: 'float32'>\n",
      "final layer output dtype:  <dtype: 'float32'> \n",
      "\n",
      "\n",
      "Batch size:  128\n",
      "\n",
      "warmup .fit run\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4631\n",
      "\n",
      "\n",
      "\n",
      "main .fit run\n",
      "39/39 [==============================] - 14s 365ms/step - loss: 2.3767\n",
      "\n",
      "warmup custom loop run\n",
      "\n",
      "\n",
      "main custom loop run\n",
      "\n",
      "[1].[2].[3].[4].[5].[6].[7].[8].[9].[10].[11].[12].[13].[14].[15].[16].[17].[18].[19].[20].[21].[22].[23].[24].[25].[26].[27].[28].[29].[30].[31].[32].[33].[34].[35].[36].[37].[38].[39].\n",
      "\n",
      ".fit method training time: 14.70\n",
      "custom training loop training time: 13.96\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow: ', tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_XLA_FLAGS'] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\"\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('float32')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "print('\\nCompute dtype: ', policy.compute_dtype)\n",
    "print('Variable dtype: ', policy.variable_dtype)\n",
    "\n",
    "def _generate_random_image(h, w):\n",
    "    image = tf.random.uniform(shape=[h, w, 3], maxval=1)\n",
    "    return image\n",
    "\n",
    "def _generate_random_label(num_classes):\n",
    "    return tf.random.uniform(shape=[], maxval=10)\n",
    "\n",
    "def _get_data(_id):\n",
    "    image = _generate_random_image(256, 256)\n",
    "    label = _generate_random_label(10)\n",
    "    return image, label\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "num_samples = 5000\n",
    "train_steps = num_samples // batch_size\n",
    "\n",
    "\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "dataset = tf.data.Dataset.range(num_samples)\n",
    "dataset = dataset.map(_get_data, num_parallel_calls=autotune) \\\n",
    "                 .batch(batch_size, drop_remainder=True) \\\n",
    "                 .repeat() \\\n",
    "                 .prefetch(autotune)\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, pooling='avg')\n",
    "x = tf.keras.layers.Dense(10)(base_model.output)\n",
    "predictions = tf.keras.layers.Activation('softmax', dtype=tf.float32)(x)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "print('\\ninternal layer output dtype: ', model.layers[21].output.dtype)\n",
    "print('final layer output dtype: ', model.output.dtype, '\\n')\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print('\\nBatch size: ', batch_size)\n",
    "\n",
    "print('\\nwarmup .fit run')\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=1)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('\\nmain .fit run')\n",
    "start_time = time()\n",
    "\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=train_steps)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "\n",
    "\n",
    "del optimizer\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "def train(num_steps):\n",
    "    \n",
    "    @tf.function(experimental_compile=True)\n",
    "    def train_step(image, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(image, training=True)\n",
    "            loss = tf.losses.sparse_categorical_crossentropy(labels, outputs)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        return loss\n",
    "    \n",
    "    print('\\nwarmup custom loop run\\n')\n",
    "    for image, labels in dataset.take(1):\n",
    "        _ = train_step(image, labels)\n",
    "\n",
    "    print('\\nmain custom loop run\\n')\n",
    "    start_time = time()\n",
    "    for step, (image, labels) in enumerate(dataset):\n",
    "        _ = train_step(image, labels)\n",
    "        print('[{}]'.format(step+1), end='.')\n",
    "        if step+1 == num_steps: break\n",
    "    end_time = time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "time = train(train_steps)\n",
    "\n",
    "print('\\n\\n.fit method training time: {:.2f}'.format(end_time - start_time))\n",
    "print('custom training loop training time: {:.2f}'.format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset kernel\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 Mixed Precision without XLA compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow:  2.2.0-rc3\n",
      "\n",
      "Compute dtype:  float16\n",
      "Variable dtype:  float32\n",
      "\n",
      "internal layer output dtype:  <dtype: 'float16'>\n",
      "final layer output dtype:  <dtype: 'float32'> \n",
      "\n",
      "\n",
      "Batch size:  256\n",
      "\n",
      "warmup .fit run\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5260\n",
      "\n",
      "\n",
      "\n",
      "main .fit run\n",
      "19/19 [==============================] - 9s 464ms/step - loss: 2.3880\n",
      "\n",
      "warmup custom loop run\n",
      "\n",
      "\n",
      "main custom loop run\n",
      "\n",
      "[1].[2].[3].[4].[5].[6].[7].[8].[9].[10].[11].[12].[13].[14].[15].[16].[17].[18].[19].\n",
      "\n",
      ".fit method training time: 9.35\n",
      "custom training loop training time: 9.31\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow: ', tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "print('\\nCompute dtype: ', policy.compute_dtype)\n",
    "print('Variable dtype: ', policy.variable_dtype)\n",
    "\n",
    "def _generate_random_image(h, w):\n",
    "    image = tf.random.uniform(shape=[h, w, 3], maxval=1)\n",
    "    return image\n",
    "\n",
    "def _generate_random_label(num_classes):\n",
    "    return tf.random.uniform(shape=[], maxval=10)\n",
    "\n",
    "def _get_data(_id):\n",
    "    image = _generate_random_image(256, 256)\n",
    "    label = _generate_random_label(10)\n",
    "    return image, label\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "num_samples = 5000\n",
    "train_steps = num_samples // batch_size\n",
    "\n",
    "\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "dataset = tf.data.Dataset.range(num_samples)\n",
    "dataset = dataset.map(_get_data, num_parallel_calls=autotune) \\\n",
    "                 .batch(batch_size, drop_remainder=True) \\\n",
    "                 .repeat() \\\n",
    "                 .prefetch(autotune)\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, pooling='avg')\n",
    "x = tf.keras.layers.Dense(10)(base_model.output)\n",
    "predictions = tf.keras.layers.Activation('softmax', dtype=tf.float32)(x)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "print('\\ninternal layer output dtype: ', model.layers[21].output.dtype)\n",
    "print('final layer output dtype: ', model.output.dtype, '\\n')\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print('\\nBatch size: ', batch_size)\n",
    "\n",
    "print('\\nwarmup .fit run')\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=1)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('\\nmain .fit run')\n",
    "start_time = time()\n",
    "\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=train_steps)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "\n",
    "\n",
    "del optimizer\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale='dynamic')\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "def train(num_steps):\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(image, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(image, training=True)\n",
    "            loss = tf.losses.sparse_categorical_crossentropy(labels, outputs)\n",
    "            scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "        scaled_grads = tape.gradient(scaled_loss, model.trainable_weights)\n",
    "        grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        return loss\n",
    "    \n",
    "    print('\\nwarmup custom loop run\\n')\n",
    "    for image, labels in dataset.take(1):\n",
    "        _ = train_step(image, labels)\n",
    "\n",
    "    print('\\nmain custom loop run\\n')\n",
    "    start_time = time()\n",
    "    for step, (image, labels) in enumerate(dataset):\n",
    "        _ = train_step(image, labels)\n",
    "        print('[{}]'.format(step+1), end='.')\n",
    "        if step+1 == num_steps: break\n",
    "    end_time = time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "time = train(train_steps)\n",
    "\n",
    "print('\\n\\n.fit method training time: {:.2f}'.format(end_time - start_time))\n",
    "print('custom training loop training time: {:.2f}'.format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset kernel\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP316 Mixed Precision with XLA compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow:  2.2.0-rc3\n",
      "\n",
      "Compute dtype:  float16\n",
      "Variable dtype:  float32\n",
      "\n",
      "internal layer output dtype:  <dtype: 'float16'>\n",
      "final layer output dtype:  <dtype: 'float32'> \n",
      "\n",
      "\n",
      "Batch size:  256\n",
      "\n",
      "warmup .fit run\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4583\n",
      "\n",
      "\n",
      "\n",
      "main .fit run\n",
      "19/19 [==============================] - 6s 309ms/step - loss: 2.3790\n",
      "\n",
      "warmup custom loop run\n",
      "\n",
      "\n",
      "main custom loop run\n",
      "\n",
      "[1].[2].[3].[4].[5].[6].[7].[8].[9].[10].[11].[12].[13].[14].[15].[16].[17].[18].[19].\n",
      "\n",
      ".fit method training time: 6.24\n",
      "custom training loop training time: 4.92\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow: ', tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_XLA_FLAGS'] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\"\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "print('\\nCompute dtype: ', policy.compute_dtype)\n",
    "print('Variable dtype: ', policy.variable_dtype)\n",
    "\n",
    "def _generate_random_image(h, w):\n",
    "    image = tf.random.uniform(shape=[h, w, 3], maxval=1)\n",
    "    return image\n",
    "\n",
    "def _generate_random_label(num_classes):\n",
    "    return tf.random.uniform(shape=[], maxval=10)\n",
    "\n",
    "def _get_data(_id):\n",
    "    image = _generate_random_image(256, 256)\n",
    "    label = _generate_random_label(10)\n",
    "    return image, label\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "num_samples = 5000\n",
    "train_steps = num_samples // batch_size\n",
    "\n",
    "\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "dataset = tf.data.Dataset.range(num_samples)\n",
    "dataset = dataset.map(_get_data, num_parallel_calls=autotune) \\\n",
    "                 .batch(batch_size, drop_remainder=True) \\\n",
    "                 .repeat() \\\n",
    "                 .prefetch(autotune)\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, pooling='avg')\n",
    "x = tf.keras.layers.Dense(10)(base_model.output)\n",
    "predictions = tf.keras.layers.Activation('softmax', dtype=tf.float32)(x)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "print('\\ninternal layer output dtype: ', model.layers[21].output.dtype)\n",
    "print('final layer output dtype: ', model.output.dtype, '\\n')\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print('\\nBatch size: ', batch_size)\n",
    "\n",
    "print('\\nwarmup .fit run')\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=1)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('\\nmain .fit run')\n",
    "start_time = time()\n",
    "\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=train_steps)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "\n",
    "\n",
    "del optimizer\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale='dynamic')\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "def train(num_steps):\n",
    "    \n",
    "    @tf.function(experimental_compile=True)\n",
    "    def train_step(image, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(image, training=True)\n",
    "            loss = tf.losses.sparse_categorical_crossentropy(labels, outputs)\n",
    "            scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "        scaled_grads = tape.gradient(scaled_loss, model.trainable_weights)\n",
    "        grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        return loss\n",
    "    \n",
    "    print('\\nwarmup custom loop run\\n')\n",
    "    for image, labels in dataset.take(1):\n",
    "        _ = train_step(image, labels)\n",
    "\n",
    "    print('\\nmain custom loop run\\n')\n",
    "    start_time = time()\n",
    "    for step, (image, labels) in enumerate(dataset):\n",
    "        _ = train_step(image, labels)\n",
    "        print('[{}]'.format(step+1), end='.')\n",
    "        if step+1 == num_steps: break\n",
    "    end_time = time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "time = train(train_steps)\n",
    "\n",
    "print('\\n\\n.fit method training time: {:.2f}'.format(end_time - start_time))\n",
    "print('custom training loop training time: {:.2f}'.format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f8e9da0c610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 553.85x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_times = {\n",
    "    'Precision': ['FP32', 'FP32', 'FP32', 'FP32', 'FP16', 'FP16', 'FP16', 'FP16'],\n",
    "    'run_type': ['.fit, xla off',\n",
    "                 'custom, xla off',\n",
    "                 '.fit, xla on',\n",
    "                 'custom, xla on',\n",
    "                 '.fit, xla off',\n",
    "                 'custom, xla off',\n",
    "                 '.fit, xla on',\n",
    "                 'custom, xla on'],\n",
    "    'times': [16.78, 16.44, 14.7, 13.96, 9.35, 9.31, 6.24, 4.92],\n",
    "}\n",
    "\n",
    "g = sns.catplot(x='Precision', y='times', hue='run_type', data=pd.DataFrame.from_dict(run_times),\n",
    "                height=6, kind=\"bar\", palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g.set_ylabels('Time in secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
